{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelltraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade vorbereitete Daten von: financial_data_prepared\\prepared_GOOG_stocks_data.csv\n",
      "Datei erfolgreich geladen. Spalten: ['Close', 'High', 'Low', 'Open', 'SMA_20', 'SMA_50', 'SMA_200', 'StdDev_20', 'Upper', 'Lower', 'RSI', 'EMA_12', 'EMA_26', 'MACD', 'Signal', 'MACD_Histogram']\n",
      "Verwende 'Close' als Preisspalte.\n",
      "Zeilen vor dropna: 1147\n",
      "Zeilen nach dropna: 1135\n",
      "Verwendete Features für das Modell (gemäß deiner Auswahl): ['Close_minus_Upper', 'Close_minus_Lower', 'Band_Width', 'Position_in_Band', 'SMA_50', 'SMA_200', 'Momentum', 'RSI', 'MACD', 'Signal', 'MACD_Histogram']\n",
      "Trainingsdaten Größe: 908, Testdaten Größe: 227\n",
      "\n",
      "Starte Hyperparameter-Optimierung (GridSearchCV)...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "Beste gefundene Parameter durch GridSearchCV: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Beste durchschnittliche Cross-Validation Accuracy: 0.5113\n",
      "\n",
      "Trainiere finales Modell mit besten Parametern auf gesamten Trainingsdaten...\n",
      "Finales Modell trainiert.\n",
      "\n",
      "Genauigkeit des OPTIMIERTEN Modells auf den Testdaten: 0.5198\n",
      "\n",
      "Feature Importance (Mean Decrease Impurity - Optimiertes Modell):\n",
      "Close_minus_Lower    0.097440\n",
      "RSI                  0.095646\n",
      "Band_Width           0.094485\n",
      "SMA_50               0.093817\n",
      "MACD_Histogram       0.092201\n",
      "SMA_200              0.092146\n",
      "Position_in_Band     0.091117\n",
      "Signal               0.089343\n",
      "MACD                 0.087648\n",
      "Momentum             0.086045\n",
      "Close_minus_Upper    0.080111\n",
      "dtype: float64\n",
      "\n",
      "Berechne Permutation Importance für optimiertes Modell (kann einen Moment dauern)...\n",
      "\n",
      "Permutation Importance (Optimiertes Modell, basierend auf Testset-Accuracy):\n",
      "  MACD_Histogram      : 0.0009 +/- 0.0113\n",
      "  RSI                 : 0.0004 +/- 0.0095\n",
      "  Momentum            : 0.0000 +/- 0.0151\n",
      "  SMA_50              : 0.0000 +/- 0.0020\n",
      "  SMA_200             : 0.0000 +/- 0.0000\n",
      "  Close_minus_Lower   : -0.0018 +/- 0.0138\n",
      "  Position_in_Band    : -0.0040 +/- 0.0189\n",
      "  MACD                : -0.0110 +/- 0.0069\n",
      "  Close_minus_Upper   : -0.0172 +/- 0.0097\n",
      "  Band_Width          : -0.0251 +/- 0.0143\n",
      "  Signal              : -0.0282 +/- 0.0175\n",
      "\n",
      "=== Skriptausführung beendet ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV # NEU: TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Konfiguration ---\n",
    "PREPARED_DATA_FOLDER = \"financial_data_prepared\"\n",
    "TICKER_SYMBOL = \"GOOG\"\n",
    "ASSET_TYPE = \"stocks\"\n",
    "\n",
    "# --- Daten laden ---\n",
    "prepared_filename = f\"prepared_{TICKER_SYMBOL}_{ASSET_TYPE}_data.csv\"\n",
    "prepared_filepath = os.path.join(PREPARED_DATA_FOLDER, prepared_filename)\n",
    "\n",
    "print(f\"Lade vorbereitete Daten von: {prepared_filepath}\")\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        prepared_filepath,\n",
    "        index_col='Date',\n",
    "        parse_dates=True\n",
    "    )\n",
    "    print(f\"Datei erfolgreich geladen. Spalten: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FEHLER: Datei nicht gefunden unter: {prepared_filepath}\")\n",
    "    print(\"Stelle sicher, dass das Haupt-Datenaufbereitungsskript erfolgreich durchlief\")\n",
    "    print(f\"und die Datei im Ordner '{os.path.dirname(prepared_file_path)}' erstellt wurde.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ein anderer Fehler trat beim Laden der Datei auf: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Preisspalte bestimmen ---\n",
    "if 'Adj Close' in df.columns:\n",
    "    price_col = 'Adj Close'\n",
    "elif 'Close' in df.columns:\n",
    "    price_col = 'Close'\n",
    "else:\n",
    "    print(\"FEHLER: Weder 'Adj Close' noch 'Close' Spalte gefunden!\")\n",
    "    exit()\n",
    "print(f\"Verwende '{price_col}' als Preisspalte.\")\n",
    "\n",
    "\n",
    "# --- Zusätzliche Features erstellen ---\n",
    "if 'Upper' in df.columns and 'Lower' in df.columns:\n",
    "    df['Close_minus_Upper'] = df[price_col] - df['Upper']\n",
    "    df['Close_minus_Lower'] = df[price_col] - df['Lower']\n",
    "    df['Band_Width'] = df['Upper'] - df['Lower']\n",
    "    df['Position_in_Band'] = ((df[price_col] - df['Lower']) / df['Band_Width']).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    print(\"Warnung: 'Upper' oder 'Lower' Spalte nicht in CSV gefunden. Bollinger-Features können nicht erstellt werden.\")\n",
    "\n",
    "df['Momentum'] = df[price_col] - df[price_col].rolling(window=12).mean()\n",
    "\n",
    "\n",
    "# --- Zielvariable definieren ---\n",
    "df['Next_Day_Price'] = df[price_col].shift(-1)\n",
    "df['Target'] = (df['Next_Day_Price'] > df[price_col]).astype(int)\n",
    "\n",
    "\n",
    "# --- Datenbereinigung ---\n",
    "print(f\"Zeilen vor dropna: {len(df)}\")\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Zeilen nach dropna: {len(df)}\")\n",
    "\n",
    "\n",
    "if df.empty:\n",
    "    print(\"FEHLER: Keine Daten mehr nach Bereinigung!\")\n",
    "    exit()\n",
    "\n",
    "# --- Features und Zielvariable definieren ---\n",
    "features = []\n",
    "potential_features = [\n",
    "    'Close_minus_Upper', 'Close_minus_Lower', 'Band_Width',\n",
    "    'Position_in_Band',\n",
    "    'SMA_50',\n",
    "    'SMA_200',\n",
    "    'Momentum',\n",
    "    # --- HIER HINZUFÜGEN ---\n",
    "    'RSI',           # RSI hinzufügen\n",
    "    'MACD',          # MACD-Linie hinzufügen\n",
    "    'Signal',        # MACD-Signallinie hinzufügen (oft auch nützlich)\n",
    "    'MACD_Histogram' # MACD-Histogramm hinzufügen (oft auch nützlich)\n",
    "    # ----------------------\n",
    "]\n",
    "\n",
    "# Nur Features hinzufügen, die im DataFrame existieren \n",
    "for feature in potential_features:\n",
    "    if feature in df.columns:\n",
    "        features.append(feature)\n",
    "    else:\n",
    "        print(f\"WARNUNG: Benötigtes Feature '{feature}' nicht im DataFrame gefunden und übersprungen!\")\n",
    "\n",
    "missing_required = [f for f in potential_features if f not in df.columns]\n",
    "if missing_required:\n",
    "     print(f\"FEHLER: Die folgenden benötigten Features fehlen im DataFrame: {missing_required}\")\n",
    "     # exit()\n",
    "\n",
    "if not features:\n",
    "    print(\"FEHLER: Keine gültigen Features für das Training gefunden!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Verwendete Features für das Modell (gemäß deiner Auswahl): {features}\")\n",
    "\n",
    "X = df[features]\n",
    "y = df['Target']\n",
    "\n",
    "# --- Daten aufteilen ---\n",
    "# Wichtig: Split VOR der Hyperparameter-Suche, damit auf dem Test-Set final validiert wird\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(f\"Trainingsdaten Größe: {len(X_train)}, Testdaten Größe: {len(X_test)}\")\n",
    "\n",
    "if len(X_train) == 0 or len(X_test) == 0:\n",
    "    print(\"FEHLER: Trainings- oder Testdatensatz ist nach dem Splitten leer.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- NEU: Hyperparameter-Optimierung ---\n",
    "print(\"\\nStarte Hyperparameter-Optimierung (GridSearchCV)...\")\n",
    "\n",
    "# 1. Parameter-Gitter definieren (Beispiel - anpassen nach Bedarf)\n",
    "#    Reduziertes Grid für schnellere Demonstration\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],             # Anzahl Bäume\n",
    "    'max_depth': [None, 10, 20],            # Max Tiefe pro Baum (None = unbegrenzt)\n",
    "    'min_samples_split': [2, 5],            # Min Samples für Split\n",
    "    'min_samples_leaf': [1, 3],             # Min Samples pro Blatt\n",
    "    'max_features': ['sqrt', None]          # Anzahl Features pro Split ('sqrt' ist oft gut)\n",
    "    # 'class_weight': [None, 'balanced']    # Optional: Bei unbalancierten Klassen testen\n",
    "}\n",
    "\n",
    "# 2. Zeitreihen-Kreuzvalidierungs-Splitter erstellen\n",
    "#    Splits die Trainingsdaten in mehrere aufeinanderfolgende Train/Validation-Folds\n",
    "tscv = TimeSeriesSplit(n_splits=5) # z.B. 5 Folds\n",
    "\n",
    "# 3. GridSearchCV-Objekt erstellen\n",
    "#    (Alternative: RandomizedSearchCV für schnellere Suche bei großen Grids)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42), # Das zu optimierende Modell\n",
    "    param_grid=param_grid,                             # Das Suchgitter\n",
    "    cv=tscv,                                           # Die Kreuzvalidierungs-Strategie\n",
    "    scoring='accuracy',                                # Die zu optimierende Metrik\n",
    "    n_jobs=-1,                                         # Nutze alle CPU-Kerne\n",
    "    verbose=1                                          # Zeige Fortschritt an\n",
    ")\n",
    "\n",
    "# 4. Suche auf den Trainingsdaten durchführen\n",
    "#    Findet die besten Parameter basierend auf der durchschnittlichen Performance\n",
    "#    über die Kreuzvalidierungs-Folds HINNERHALB der Trainingsdaten.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Beste gefundene Parameter und bestes Modell extrahieren\n",
    "print(f\"\\nBeste gefundene Parameter durch GridSearchCV: {grid_search.best_params_}\")\n",
    "print(f\"Beste durchschnittliche Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "best_model = grid_search.best_estimator_ # Das ist das RandomForest-Modell mit den besten Parametern\n",
    "\n",
    "# --- Modell trainieren ---\n",
    "# Dieser Schritt ist jetzt implizit durch grid_search.fit() passiert.\n",
    "# best_model ist bereits auf den (besten Teilen der) Trainingsdaten trainiert.\n",
    "# Man könnte optional best_model.fit(X_train, y_train) erneut aufrufen, um sicherzustellen,\n",
    "# dass es auf den *gesamten* Trainingsdaten mit den besten Parametern trainiert ist.\n",
    "# Das ist oft gängige Praxis nach einer CV-Suche.\n",
    "print(\"\\nTrainiere finales Modell mit besten Parametern auf gesamten Trainingsdaten...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Finales Modell trainiert.\")\n",
    "\n",
    "\n",
    "# --- Vorhersagen machen (mit dem optimierten Modell) ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# --- Bewertung (des optimierten Modells) ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nGenauigkeit des OPTIMIERTEN Modells auf den Testdaten: {accuracy:.4f}')\n",
    "\n",
    "# --- Feature Importance (vom optimierten Modell) ---\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nFeature Importance (Mean Decrease Impurity - Optimiertes Modell):\")\n",
    "    if len(features) == best_model.n_features_in_:\n",
    "         mdi_importances = pd.Series(best_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "         print(mdi_importances)\n",
    "    else:\n",
    "        print(\"Warnung: Anzahl der Features stimmt nicht mit Modell überein, MDI Importance wird nicht angezeigt.\")\n",
    "\n",
    "# --- Permutation Importance (vom optimierten Modell) ---\n",
    "print(\"\\nBerechne Permutation Importance für optimiertes Modell (kann einen Moment dauern)...\")\n",
    "perm_importance_result = permutation_importance(\n",
    "    estimator=best_model, # BENUTZE DAS OPTIMIERTE MODELL\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    scoring='accuracy',\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ergebnisse aufbereiten und anzeigen\n",
    "perm_sorted_idx = perm_importance_result.importances_mean.argsort()[::-1]\n",
    "\n",
    "print(\"\\nPermutation Importance (Optimiertes Modell, basierend auf Testset-Accuracy):\")\n",
    "for i in perm_sorted_idx:\n",
    "    feature_name = features[i]\n",
    "    mean_importance = perm_importance_result.importances_mean[i]\n",
    "    std_importance = perm_importance_result.importances_std[i]\n",
    "    print(f\"  {feature_name:<20}: {mean_importance:.4f} +/- {std_importance:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Skriptausführung beendet ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
